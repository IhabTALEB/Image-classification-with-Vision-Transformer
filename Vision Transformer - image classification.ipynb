{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification - Vision Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uRPhOfJQyxzC",
        "InDCMr5tnCrM",
        "1J3Lvghf2FS8",
        "mhT7tN-cysTF",
        "tONDgDOPRXUJ",
        "qlb7n_wYpMBf",
        "3P_JIcUTgOMW",
        "96Dvyi2tjizp",
        "sNJMIHxtokgq",
        "WN4I8HZqRqC3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRPhOfJQyxzC"
      },
      "source": [
        "# Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e44TPMq0nQSO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InDCMr5tnCrM"
      },
      "source": [
        "# Imports, Constants and Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3Lvghf2FS8"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Xmx_9Dlzn2",
        "cellView": "form"
      },
      "source": [
        "#@markdown Dataset\n",
        "#@markdown ---\n",
        "IMG_DIMS = 72 #@param {type:\"slider\", min:16, max:256, step:1}\n",
        "NUM_CHANS = 3 #@param {type:\"slider\", min:1, max:4, step:1}\n",
        "#number maximum of images used from the dataset (For small RAMs...)\n",
        "MAX_DATA_SIZE = 7000 #@param {type:\"slider\", min:4000, max:35000, step:1000}\n",
        "NUM_CLASSES = 8 #@param {type:\"slider\", min:2, max:20, step:1}\n",
        "SPLIT_TRAIN_TEST = True #@param {type:\"boolean\"}\n",
        "TRAIN_RATIO = 95 #@param {type:\"slider\", min:60, max:95, step:5}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown Resizing\n",
        "#@markdown ---\n",
        "SAVE_RESIZED = True #@param {type:\"boolean\"}\n",
        "RESIZED_DIR = \"/content/natural-images\" #@param {type:\"string\"}\n",
        "RESIZED_X=RESIZED_DIR + \"train_x_{}_{}.npy\".format(IMG_DIMS, MAX_DATA_SIZE)\n",
        "RESIZED_Y=RESIZED_DIR + \"train_y_{}_{}.npy\".format(IMG_DIMS, MAX_DATA_SIZE)\n",
        "RESIZED_PATH={\n",
        "    'x': RESIZED_X,\n",
        "    'y': RESIZED_Y\n",
        "}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "#@markdown Hyperparameters\n",
        "#@markdown ---\n",
        "#default: 50\n",
        "NUM_EPOCHS = 50 #@param {type:\"slider\", min:10, max:300, step:5}\n",
        "#default: 128\n",
        "BATCH_SIZE = 256 #@param {type:\"slider\", min:32, max:512, step:32}\n",
        "INIT_LR = 0.001 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown Benchmarks, tests and Outputs\n",
        "#@markdown --\n",
        "IMG_DIR = \"/content/natural-images/data\" #@param {type:\"string\"}\n",
        "OUTPUT_PATH = \"/content/gdrive/MyDrive/Colab Files/vision transformer/classification/natural-images/\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "#@markdown Transformer Parameters\n",
        "#@markdown --\n",
        "patch_size = 6 #@param {type:\"slider\", min:2, max:20, step:1} # Size of the patches to be extract from the input images\n",
        "projection_dim = 64 #@param {type:\"slider\", min:16, max:96, step:16}\n",
        "num_heads = 4 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "transformer_layers = 8 #@param {type:\"slider\", min:4, max:16, step:1}\n",
        "\n",
        "# weight_decay = 0.0001\n",
        "num_patches = (IMG_DIMS // patch_size) ** 2\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhT7tN-cysTF"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dXixNIggroG"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "K.clear_session()\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "import random\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "from tensorflow.keras.layers import Add\n",
        "from tensorflow.keras import Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow.keras\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tONDgDOPRXUJ"
      },
      "source": [
        "## Custom Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0TvUe0emgnL"
      },
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COJqshz5Ijs5"
      },
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x056JCsELJnV"
      },
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = Dense(units=projection_dim)\n",
        "        self.position_embedding = Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlb7n_wYpMBf"
      },
      "source": [
        "## Prepare Kaggle-cli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzzeSFfSnDnN"
      },
      "source": [
        "%%shell\n",
        "\n",
        "#!/bin/bash\n",
        "if [ -e \"~/.kaggle/kaggle.json\" ]\n",
        "then\n",
        "\techo \"kaggle.json already exists\"\n",
        "\techo \"Skipping to next cell...\"\n",
        "else\n",
        "\techo \"Preparing Kaggle-cli...\"\n",
        "\tpip install -q kaggle\n",
        "\tpip install -q kaggle-cli\n",
        "\n",
        "\tmkdir -p ~/.kaggle\n",
        "\tcp \"/content/gdrive/MyDrive/Colab Files/Kaggle/kaggle.json\" ~/.kaggle/\n",
        "\tcat ~/.kaggle/kaggle.json \n",
        "\tchmod 600 ~/.kaggle/kaggle.json\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P_JIcUTgOMW"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Dvyi2tjizp"
      },
      "source": [
        "## Download and Unzip The DataSet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-bFNwDWjy4u"
      },
      "source": [
        "%%shell\n",
        "\n",
        "#!/bin/bash\n",
        "if [ -e \"/content/natural-images.zip\" ]\n",
        "then\n",
        "\t\techo \"Dataset already downloaded\"\n",
        "\t\techo \"Skipping to next cell...\"\n",
        "else\n",
        "\t\techo \"Downloading Dataset...\"\n",
        "    kaggle datasets download -d prasunroy/natural-images\n",
        "\t\techo \"Unzipping Dataset...\"\n",
        "\t\tunzip \"/content/natural-images.zip\" -d \"/content/natural-images\"\n",
        "    echo \"Done.\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNJMIHxtokgq"
      },
      "source": [
        "## Prepare and Save The DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOqbQtPY0IF1"
      },
      "source": [
        "def prepare_dataset(ds_exists = False, img_dim = IMG_DIMS, save_resized = SAVE_RESIZED, resized_path = RESIZED_PATH,\n",
        "                    max_data_size = MAX_DATA_SIZE, split_train_test = SPLIT_TRAIN_TEST, train_ratio = TRAIN_RATIO):\n",
        "  if ds_exists:\n",
        "    totalImages = np.load(resized_path['x'])\n",
        "    totalY = np.load(resized_path['y'])\n",
        "    if split_train_test:\n",
        "      train_test=int(max_data_size*(train_ratio/100))\n",
        "      print (\"train/test = \", train_test, \"/\", totalImages.shape[0]-train_test)\n",
        "      return (totalImages[:train_test,:,:,:], totalY[:train_test]), (totalImages[train_test:,:,:,:], totalY[train_test:])\n",
        "    else:\n",
        "      return (totalImages, totalY), (np.array(), np.array())\n",
        "  else:\n",
        "    images=[]\n",
        "    y=[]\n",
        "    length=1\n",
        "    dim=0\n",
        "    all_img_paths = [os.path.join(dp, f) for dp, dn, fn in os.walk(IMG_DIR) for f in fn]\n",
        "    random.shuffle(all_img_paths)\n",
        "    for image_path in all_img_paths:\n",
        "      img = cv2.imread(image_path)\n",
        "      resized = cv2.resize(img,(img_dim,img_dim))\n",
        "      images.append(resized)\n",
        "      y.append(os.path.dirname(image_path))\n",
        "      length += 1\n",
        "      if length>max_data_size:\n",
        "        break\n",
        "    del all_img_paths\n",
        "    totalImages=np.array(images)\n",
        "    y_array=np.array(y)\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_enc = label_encoder.fit_transform(y_array)\n",
        "    totalY = y_enc\n",
        "    del images\n",
        "    totalImages = totalImages.astype(\"float\")/255\n",
        "    if save_resized:\n",
        "      np.save(resized_path['x'],totalImages)\n",
        "      np.save(resized_path['y'],totalY)\n",
        "      print(\"Resized images saved in path: \", RESIZED_PATH)\n",
        "    if split_train_test:\n",
        "      train_test=int(max_data_size*(train_ratio/100))\n",
        "      print (\"train/test = \", train_test, \"/\", max_data_size-train_test)\n",
        "      return (totalImages[:train_test,:,:,:], totalY[:train_test]), (totalImages[train_test:,:,:,:], totalY[train_test:])\n",
        "    else:\n",
        "      return (totalImages, totalY), (np.array(), np.array())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Baq6DrD8olNc"
      },
      "source": [
        "if os.path.isfile(RESIZED_PATH['y']):\n",
        "\tprint(\".npy file already exists\")\n",
        "\tprint(\"Reading the existing file...\")\n",
        "\t(x_train, y_train), (x_test, y_test) = prepare_dataset(ds_exists = True)\n",
        "else:\n",
        "\t(x_train, y_train), (x_test, y_test) = prepare_dataset()\n",
        "\tprint(\"Dataset prepared\")\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4I8HZqRqC3"
      },
      "source": [
        "# Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LingafjgLM9G"
      },
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = Input((IMG_DIMS, IMG_DIMS, NUM_CHANS))\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = Flatten()(representation)\n",
        "    representation = Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = Dense(NUM_CLASSES)(features)\n",
        "    # Create the Keras model.\n",
        "    model = Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaRHP2zySFYu"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwzgK5ZNLP0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5d214ac-722b-410f-abd7-b6e9514475b8"
      },
      "source": [
        "transformer = create_vit_classifier()\n",
        "\n",
        "transformer.compile(\n",
        "    optimizer=Adam(lr=INIT_LR),\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\n",
        "             keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")#,\n",
        "    ],\n",
        ")\n",
        "history = transformer.fit(x=x_train, y=y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, shuffle=True, validation_split=0.1,)\n",
        "print(\"\\nEvaluating System...\")\n",
        "_, accuracy = transformer.evaluate(x_test, y_test)\n",
        "print(\"Accuracy: \", round(accuracy * 100, 2), \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "24/24 [==============================] - 31s 876ms/step - loss: 6.7405 - accuracy: 0.1790 - val_loss: 1.7590 - val_accuracy: 0.2932\n",
            "Epoch 2/50\n",
            "24/24 [==============================] - 19s 808ms/step - loss: 1.8617 - accuracy: 0.2845 - val_loss: 1.4814 - val_accuracy: 0.4556\n",
            "Epoch 3/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 1.5219 - accuracy: 0.4155 - val_loss: 1.0854 - val_accuracy: 0.6406\n",
            "Epoch 4/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 1.2294 - accuracy: 0.5556 - val_loss: 0.8336 - val_accuracy: 0.7083\n",
            "Epoch 5/50\n",
            "24/24 [==============================] - 19s 808ms/step - loss: 0.9465 - accuracy: 0.6680 - val_loss: 0.6223 - val_accuracy: 0.7880\n",
            "Epoch 6/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.7764 - accuracy: 0.7275 - val_loss: 0.5358 - val_accuracy: 0.8271\n",
            "Epoch 7/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.6648 - accuracy: 0.7640 - val_loss: 0.4907 - val_accuracy: 0.8331\n",
            "Epoch 8/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.5743 - accuracy: 0.7891 - val_loss: 0.4790 - val_accuracy: 0.8180\n",
            "Epoch 9/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.5453 - accuracy: 0.7933 - val_loss: 0.4332 - val_accuracy: 0.8301\n",
            "Epoch 10/50\n",
            "24/24 [==============================] - 19s 805ms/step - loss: 0.4442 - accuracy: 0.8383 - val_loss: 0.3652 - val_accuracy: 0.8632\n",
            "Epoch 11/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.3999 - accuracy: 0.8560 - val_loss: 0.3668 - val_accuracy: 0.8602\n",
            "Epoch 12/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.3336 - accuracy: 0.8742 - val_loss: 0.3684 - val_accuracy: 0.8692\n",
            "Epoch 13/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.2919 - accuracy: 0.8911 - val_loss: 0.3827 - val_accuracy: 0.8737\n",
            "Epoch 14/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.2705 - accuracy: 0.8947 - val_loss: 0.3815 - val_accuracy: 0.8647\n",
            "Epoch 15/50\n",
            "24/24 [==============================] - 19s 805ms/step - loss: 0.2713 - accuracy: 0.8976 - val_loss: 0.3912 - val_accuracy: 0.8662\n",
            "Epoch 16/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.2618 - accuracy: 0.9004 - val_loss: 0.3797 - val_accuracy: 0.8707\n",
            "Epoch 17/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.2040 - accuracy: 0.9239 - val_loss: 0.3440 - val_accuracy: 0.8752\n",
            "Epoch 18/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.1745 - accuracy: 0.9336 - val_loss: 0.3248 - val_accuracy: 0.8767\n",
            "Epoch 19/50\n",
            "24/24 [==============================] - 19s 805ms/step - loss: 0.1463 - accuracy: 0.9475 - val_loss: 0.3283 - val_accuracy: 0.8992\n",
            "Epoch 20/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.1378 - accuracy: 0.9521 - val_loss: 0.4151 - val_accuracy: 0.8797\n",
            "Epoch 21/50\n",
            "24/24 [==============================] - 19s 808ms/step - loss: 0.1211 - accuracy: 0.9536 - val_loss: 0.3512 - val_accuracy: 0.8917\n",
            "Epoch 22/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.1114 - accuracy: 0.9580 - val_loss: 0.3867 - val_accuracy: 0.8917\n",
            "Epoch 23/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.1169 - accuracy: 0.9609 - val_loss: 0.3689 - val_accuracy: 0.8872\n",
            "Epoch 24/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.0976 - accuracy: 0.9655 - val_loss: 0.3731 - val_accuracy: 0.8902\n",
            "Epoch 25/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0833 - accuracy: 0.9720 - val_loss: 0.3946 - val_accuracy: 0.8737\n",
            "Epoch 26/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.1030 - accuracy: 0.9647 - val_loss: 0.4409 - val_accuracy: 0.8857\n",
            "Epoch 27/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0966 - accuracy: 0.9671 - val_loss: 0.4090 - val_accuracy: 0.8902\n",
            "Epoch 28/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.0863 - accuracy: 0.9707 - val_loss: 0.3675 - val_accuracy: 0.8902\n",
            "Epoch 29/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.0645 - accuracy: 0.9761 - val_loss: 0.4267 - val_accuracy: 0.8707\n",
            "Epoch 30/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0608 - accuracy: 0.9771 - val_loss: 0.4526 - val_accuracy: 0.8812\n",
            "Epoch 31/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.0844 - accuracy: 0.9737 - val_loss: 0.3642 - val_accuracy: 0.8782\n",
            "Epoch 32/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0841 - accuracy: 0.9746 - val_loss: 0.3915 - val_accuracy: 0.8932\n",
            "Epoch 33/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0657 - accuracy: 0.9741 - val_loss: 0.3752 - val_accuracy: 0.8857\n",
            "Epoch 34/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0554 - accuracy: 0.9799 - val_loss: 0.4533 - val_accuracy: 0.8872\n",
            "Epoch 35/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.0571 - accuracy: 0.9821 - val_loss: 0.4390 - val_accuracy: 0.9023\n",
            "Epoch 36/50\n",
            "24/24 [==============================] - 19s 808ms/step - loss: 0.0438 - accuracy: 0.9830 - val_loss: 0.4524 - val_accuracy: 0.8947\n",
            "Epoch 37/50\n",
            "24/24 [==============================] - 19s 809ms/step - loss: 0.0499 - accuracy: 0.9810 - val_loss: 0.4689 - val_accuracy: 0.8977\n",
            "Epoch 38/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0651 - accuracy: 0.9776 - val_loss: 0.4840 - val_accuracy: 0.8842\n",
            "Epoch 39/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0593 - accuracy: 0.9811 - val_loss: 0.3874 - val_accuracy: 0.8932\n",
            "Epoch 40/50\n",
            "24/24 [==============================] - 19s 808ms/step - loss: 0.0651 - accuracy: 0.9784 - val_loss: 0.5056 - val_accuracy: 0.8857\n",
            "Epoch 41/50\n",
            "24/24 [==============================] - 19s 808ms/step - loss: 0.0621 - accuracy: 0.9805 - val_loss: 0.4247 - val_accuracy: 0.8917\n",
            "Epoch 42/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.0458 - accuracy: 0.9854 - val_loss: 0.4310 - val_accuracy: 0.8992\n",
            "Epoch 43/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.0862 - accuracy: 0.9742 - val_loss: 0.4866 - val_accuracy: 0.8872\n",
            "Epoch 44/50\n",
            "24/24 [==============================] - 19s 806ms/step - loss: 0.0601 - accuracy: 0.9798 - val_loss: 0.3821 - val_accuracy: 0.8887\n",
            "Epoch 45/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0695 - accuracy: 0.9773 - val_loss: 0.4329 - val_accuracy: 0.8812\n",
            "Epoch 46/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0543 - accuracy: 0.9842 - val_loss: 0.4083 - val_accuracy: 0.8827\n",
            "Epoch 47/50\n",
            "24/24 [==============================] - 19s 812ms/step - loss: 0.0539 - accuracy: 0.9828 - val_loss: 0.6040 - val_accuracy: 0.8692\n",
            "Epoch 48/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0638 - accuracy: 0.9776 - val_loss: 0.4215 - val_accuracy: 0.8872\n",
            "Epoch 49/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0454 - accuracy: 0.9873 - val_loss: 0.4482 - val_accuracy: 0.8917\n",
            "Epoch 50/50\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.0406 - accuracy: 0.9859 - val_loss: 0.3936 - val_accuracy: 0.8917\n",
            "\n",
            "Evaluating System...\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 0.3105 - accuracy: 0.9277\n",
            "Accuracy:  92.77 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}